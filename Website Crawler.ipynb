{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import schedule\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "article_links = [] \n",
    "titles = []\n",
    "names = []\n",
    "dates = [] \n",
    "Visited_webpages_page = []\n",
    "Visited_webpages_authors = []\n",
    "profiles = []\n",
    "profile_urls = []\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36'}\n",
    "\n",
    "#profile collection\n",
    "\n",
    "def scholar_spider():\n",
    "    '''Function to crawler google scholar'''\n",
    "    max_pages = 1\n",
    "    start_pages = 0\n",
    "    max_article = \"0 - 0\"\n",
    "    after_a =  \" \"\n",
    "    after_author = \" \"\n",
    "    item_url = \" \"\n",
    "    count = 0  \n",
    "    page = 1 \n",
    "    url = 'y'\n",
    "    url1 = 'y'\n",
    "    while page >= max_pages:   \n",
    "        print('page number is:', page)\n",
    "        print('max page is:', max_pages)\n",
    "        \n",
    "  \n",
    "        start_pages = (max_article.split()[2])\n",
    "        url = 'https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779' + str(\"&after_author=\") + str(after_author) + str('&astart=') + str(start_pages) #allows url to change as it traverses through each webpage\n",
    "        \n",
    "        print('url is', url)\n",
    "        Visited_webpages_page.append(url)\n",
    "        time.sleep(random.uniform(10,15))\n",
    "        source_code = requests.get(url, headers = headers)\n",
    "        plain_text = source_code.text \n",
    "        soup =BeautifulSoup(plain_text) \n",
    "        \n",
    "        #profile links\n",
    "        \n",
    "        for link in soup.findAll('a', {'class': 'gs_ai_pho'}):    \n",
    "            profile = 'https://scholar.google.co.uk' + link.get('href')\n",
    "            profile_urls.append(profile)\n",
    "        for link in soup.findAll('button', {'class' : 'gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx'}):\n",
    "            after_a = link.get('onclick')\n",
    "            # breaks while loop when no next page\n",
    "            if after_a == None:\n",
    "                max_pages = page + 1\n",
    "                page = 0\n",
    "            else:\n",
    "                after_author = after_a[109:121]\n",
    "                \n",
    "                for link in soup.findAll('span', {'class': 'gs_nph gsc_pgn_ppn'}):\n",
    "                    max_article = link.string\n",
    "                    max_pages = (max_article.split())[2]\n",
    "                    max_pages = int(int(max_pages)/10)+1    \n",
    "                    print(max_pages)\n",
    "                page += 1\n",
    "        print('max pages at end', max_pages)\n",
    "                \n",
    "        print('pages at end:', page)\n",
    "    print(access_all())\n",
    "    \n",
    "       \n",
    "        \n",
    "# article document information\n",
    "\n",
    "def access_all():\n",
    "    '''function to extract document information'''\n",
    "    for p in profile_urls:\n",
    "        startpage = 0\n",
    "        item_url1= 'yes'\n",
    "        item_url2 = 'yes'\n",
    "        print(p)\n",
    "        while item_url1 == item_url2:\n",
    "            item_url1 = p + str('&cstart=') + str(startpage) + str('&pagesize=100') \n",
    "            Visited_webpages_authors.append(item_url1)  \n",
    "            source_code = requests.get(item_url1, headers=headers)\n",
    "            plain_text = source_code.text\n",
    "            soup =BeautifulSoup(plain_text)\n",
    "            #scrapes links and titles\n",
    "            for article in soup.findAll('a', {'class': 'gsc_a_at'}):\n",
    "                article_links.append('https://scholar.google.co.uk' + article.get('data-href'))\n",
    "                titles.append(article.string)\n",
    "            #scrapes dates\n",
    "            for link in soup.findAll('span', {'class':'gsc_a_h gsc_a_hc gs_ibl'}):\n",
    "                date = (link.string)\n",
    "                dates.append(date)\n",
    "                #scrapes authors\n",
    "                for link in soup.findAll('div', {'id': 'gsc_prf_in'}):\n",
    "                    names.append(link.string)\n",
    "            for link in soup.findAll('span', {'id': 'gsc_a_nn'}):\n",
    "                article_number = link.string\n",
    "                print(article_number)\n",
    "                startpage = int(article_number.split(\"â€“\",1)[1])\n",
    "                item_url2 = item_url1\n",
    "                \n",
    "   \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#schedule once a week\n",
    "\n",
    "schedule.every().thursday.at(\"17:02\").do(scholar_spider)\n",
    "\n",
    "\n",
    "#must remain open to stop shell ending\n",
    "while True:\n",
    "    #turns numbers into strings\n",
    "    titles1 = list( map(str, titles) )\n",
    "    dates1 = list( map(str, dates) )\n",
    "    titles = titles1\n",
    "    dates = dates1\n",
    "    \n",
    "#main document list    \n",
    "    aa = [x + str(\" \") + c + str(\" \") + z for x, c, z in zip(titles, dates, names)]\n",
    "    \n",
    "    \n",
    "    Dic1 = dict(list(enumerate(article_links)))\n",
    "    Dic2 = dict(list(enumerate(aa)))\n",
    "    Dic3 = dict(list(enumerate(titles)))\n",
    "    Dic4 = dict(list(enumerate(dates)))\n",
    "    Dic5 = dict(list(enumerate(names)))\n",
    "    \n",
    "    \n",
    "#save dictionaries to a file \n",
    "    with open('aa', 'wb') as fp:\n",
    "        pickle.dump(aa, fp)\n",
    "\n",
    "    with open('Dic5', 'wb') as fp:\n",
    "        pickle.dump(Dic5, fp)    \n",
    "\n",
    "\n",
    "    with open('Dic3', 'wb') as fp:\n",
    "        pickle.dump(Dic3, fp)    \n",
    "    \n",
    "    with open('Dic4', 'wb') as fp:\n",
    "        pickle.dump(Dic4, fp)\n",
    "    \n",
    "    with open('Dic2', 'wb') as fp:\n",
    "        pickle.dump(Dic2, fp)\n",
    "\n",
    "    with open('Dic1', 'wb') as fp:\n",
    "        pickle.dump(Dic1, fp)\n",
    "    \n",
    "    schedule.run_pending()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
